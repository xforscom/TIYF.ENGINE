name: dataqa-tolerance

on:
  workflow_dispatch:
  push:
    branches: [ main, fix/dataqa-tolerance-missing-bars ]
  pull_request:
    branches: [ main ]

jobs:
  matrix:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup .NET 8
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '8.0.x'

      - name: Setup Node.js 20
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Build (Release)
        run: dotnet build -c Release

      - name: Prepare paths
        id: prep
        shell: bash
        run: |
          set -euo pipefail
          ROOT="$(pwd)"
          echo "root=$ROOT" >> $GITHUB_OUTPUT
          SIM_DLL="$ROOT/src/TiYf.Engine.Sim/bin/Release/net8.0/TiYf.Engine.Sim.dll"
          TOOLS_REL="$ROOT/src/TiYf.Engine.Tools/bin/Release/net8.0/TiYf.Engine.Tools.dll"
          if [ ! -f "$SIM_DLL" ]; then
            echo "Sim Release DLL not found: $SIM_DLL" >&2
            exit 1
          fi
          if [ ! -f "$TOOLS_REL" ]; then
            echo "Tools Release DLL not found: $TOOLS_REL" >&2
            ls -R "$ROOT/src/TiYf.Engine.Tools/bin" || true
            exit 1
          fi
          echo "sim=$SIM_DLL" >> $GITHUB_OUTPUT
          echo "tools=$TOOLS_REL" >> $GITHUB_OUTPUT
          echo "cfg=$ROOT/tests/fixtures/backtest_m0/config.backtest-m0.json" >> $GITHUB_OUTPUT

      - name: Prepare artifact dirs
        shell: bash
        run: |
          set -euo pipefail
          rm -rf artifacts || true
          mkdir -p artifacts/dataqa/pass artifacts/dataqa/fail artifacts/env

      - name: Run K=1 (pass)
        shell: bash
        run: |
          set -euo pipefail
          root='${{ steps.prep.outputs.root }}'
          sim='${{ steps.prep.outputs.sim }}'
          tools='${{ steps.prep.outputs.tools }}'
          cfg='${{ steps.prep.outputs.cfg }}'
          tmp="$root/.tmp_k1_pass"
          rm -rf "$tmp" && mkdir -p "$tmp"
          export CONFIG_IN="$cfg"
          export CONFIG_OUT="$tmp/config.k1pass.json"
          export TMP_DIR="$tmp"
          export K_MODE="pass"
          export INSTR_FILE="$root/tests/fixtures/backtest_m0/instruments.csv"
          # Node-based preparation: copy ticks, remove 1 data row from each, rewrite config with absolute paths and temp journalDir, force dataQa active
          node - <<'NODE'
          const fs = require('fs');
          const path = require('path');
          const configIn = process.env.CONFIG_IN;
          const configOut = process.env.CONFIG_OUT;
          const tmpDir = process.env.TMP_DIR;
          const kMode = process.env.K_MODE || 'pass';
          const instrumentsFile = process.env.INSTR_FILE;
          if (!configIn || !configOut || !tmpDir || !instrumentsFile) throw new Error('Missing env CONFIG_IN/CONFIG_OUT/TMP_DIR/INSTR_FILE');
          fs.mkdirSync(tmpDir, { recursive: true });
          const cfg = JSON.parse(fs.readFileSync(configIn, 'utf8'));
          // Ensure dataQa active and K=1
          cfg.featureFlags = { ...(cfg.featureFlags||{}), dataQa: 'active' };
          cfg.dataQA = { enabled: true, maxMissingBarsPerInstrument: 1, allowDuplicates: false, spikeZ: 5, repair: { forwardFillBars: 0, dropSpikes: true } };
          // Rewrite instrumentsFile absolute
          cfg.data = cfg.data || {};
          cfg.data.instrumentsFile = path.resolve(instrumentsFile);
          // Process ticks
          if (!cfg.data.ticks || typeof cfg.data.ticks !== 'object') throw new Error('config.data.ticks missing or not an object');
          const rewritten = {};
          // For overflow: create a genuine per-symbol hole: remove 2 from EURUSD only; others remove 0
          const removeNFor = (sym) => (kMode === 'overflow' ? (sym === 'EURUSD' ? 2 : 0) : 1);
          for (const [sym, srcRaw] of Object.entries(cfg.data.ticks)) {
            if (typeof srcRaw !== 'string') continue;
            const src = path.resolve(srcRaw);
            if (!fs.existsSync(src)) throw new Error(`Tick file not found: ${src}`);
            const dst = path.join(tmpDir, `ticks_${sym}.csv`);
            const lines = fs.readFileSync(src, 'utf8').split(/\r?\n/);
            const header = lines[0] ?? '';
            const data = lines.slice(1).filter(l => l.trim().length > 0);
            const toRemove = Math.min(removeNFor(sym), data.length);
            const dataAfter = data.slice(toRemove);
            const out = [header, ...dataAfter].join('\n') + '\n';
            fs.writeFileSync(dst, out, 'utf8');
            // Verify file exists and has >=2 lines (header + at least 1 data row)
            const verify = fs.readFileSync(dst, 'utf8').trim().split(/\r?\n/);
            if (verify.length < 2) {
              console.error(`After K edit, ${dst} has no data rows (lines=${verify.length}).`);
              process.exit(1);
            }
            rewritten[sym] = path.resolve(dst);
          }
          cfg.data.ticks = rewritten;
          // Set temp journalDir
          cfg.output = cfg.output || {};
          cfg.output.journalDir = path.resolve(path.join(tmpDir, 'journals'));
          // Ensure name is backtest-m0 for engine heuristics
          cfg.name = cfg.name || 'backtest-m0';
          fs.writeFileSync(configOut, JSON.stringify(cfg, null, 2), 'utf8');
          // Export CONFIG_OUT for subsequent steps
          fs.appendFileSync(process.env.GITHUB_ENV, `\nCONFIG_OUT=${configOut}\n`);
          fs.appendFileSync(process.env.GITHUB_ENV, `TMP_DIR=${tmpDir}\n`);
          NODE
          # Debug: list temp dir and first 3 lines of each rewritten CSV
          echo "Temp dir listing:" && ls -la "$TMP_DIR"
          for f in "$TMP_DIR"/ticks_*.csv; do echo "== $f =="; head -n 3 "$f"; done
          # Run sim with prepared config and capture stdout for precise journal paths
          OUT=$(mktemp)
          set +e
          dotnet exec "$sim" --config "$CONFIG_OUT" --run-id K1-PASS --verbosity quiet | tee "$OUT"
          SIM_CODE=${PIPESTATUS[0]}
          set -e
          if [ $SIM_CODE -ne 0 ]; then
            echo "Simulator exited with $SIM_CODE" >&2
            echo '--- simulator stdout (K1-PASS) ---'
            cat "$OUT"
            exit $SIM_CODE
          fi
          J_EVENTS=$(grep -m1 '^JOURNAL_DIR_EVENTS=' "$OUT" | cut -d= -f2 || true)
          J_TRADES=$(grep -m1 '^JOURNAL_DIR_TRADES=' "$OUT" | cut -d= -f2 || true)
          if [ -z "$J_EVENTS" ] || [ -z "$J_TRADES" ]; then
            echo "Missing JOURNAL_DIR_* entries for K1-PASS" >&2
            echo '--- simulator stdout (K1-PASS) ---'
            cat "$OUT"
            exit 2
          fi
          test -f "$J_EVENTS" || (echo "events missing: $J_EVENTS" >&2; exit 1)
          test -f "$J_TRADES" || (echo "trades missing: $J_TRADES" >&2; exit 1)
          cp "$OUT" artifacts/dataqa/pass/sim.log
          printf 'mode=pass\nrun_dir=%s\nevents=%s\ntrades=%s\n' "$(dirname "$J_EVENTS")" "$J_EVENTS" "$J_TRADES" >> artifacts/env/env.sanity
          cp "$J_EVENTS" artifacts/dataqa/pass/events.csv
          cp "$J_TRADES" artifacts/dataqa/pass/trades.csv
          # Assert summary passed=true and no abort (pure Node)
          EVENTS="$J_EVENTS" node - <<'JS'
          const fs=require("fs");
          const line=(fs.readFileSync(process.env.EVENTS,"utf8").split(/\r?\n/).find(l=>l.includes(",DATA_QA_SUMMARY_V1,"))||"");
          if(!line){ console.error("DATA_QA_SUMMARY_V1 not found"); process.exit(1); }
          const QUOTE=String.fromCharCode(34);
          function splitCsv(s){ const out=[]; let buf="", q=false; for(let i=0;i<s.length;i++){ const c=s[i]; if(q){ if(c===QUOTE){ if(i+1<s.length && s[i+1]===QUOTE){ buf+=QUOTE; i++; } else { q=false; } } else { buf+=c; } } else { if(c===','){ out.push(buf); buf=""; } else if(c===QUOTE){ q=true; } else { buf+=c; } } } out.push(buf); return out; }
          const cols=splitCsv(line);
          const payload=cols[3];
          try {
            const j=JSON.parse(payload);
            if(!(j.passed===true && j.aborted===false)){ console.error("Unexpected DATA_QA_SUMMARY_V1", j); process.exit(1); }
          } catch(e) {
            console.error("Failed to parse payload as JSON:", payload); process.exit(1);
          }
          JS
          EVENTS="$J_EVENTS" node - <<'JS'
          const fs=require('fs');
          const text=fs.readFileSync(process.env.EVENTS,'utf8');
          if (text.includes(',DATA_QA_ABORT_V1,')) { console.error('Unexpected abort event found'); process.exit(1); }
          JS
          # Strict verify (deep)
          dotnet exec "$tools" verify strict --events "$J_EVENTS" --trades "$J_TRADES" --schema 1.3.0 --json

      - name: Run K=1 with overflow (fail)
        shell: bash
        run: |
          set -euo pipefail
          root='${{ steps.prep.outputs.root }}'
          sim='${{ steps.prep.outputs.sim }}'
          tools='${{ steps.prep.outputs.tools }}'
          cfg='${{ steps.prep.outputs.cfg }}'
          tmp="$root/.tmp_k1_over"
          rm -rf "$tmp" && mkdir -p "$tmp"
          export CONFIG_IN="$cfg"
          export CONFIG_OUT="$tmp/config.k1over.json"
          export TMP_DIR="$tmp"
          export K_MODE="overflow"
          export INSTR_FILE="$root/tests/fixtures/backtest_m0/instruments.csv"
          node - <<'NODE'
          const fs = require('fs');
          const path = require('path');
          const configIn = process.env.CONFIG_IN;
          const configOut = process.env.CONFIG_OUT;
          const tmpDir = process.env.TMP_DIR;
          const kMode = process.env.K_MODE || 'overflow';
          const instrumentsFile = process.env.INSTR_FILE;
          if (!configIn || !configOut || !tmpDir || !instrumentsFile) throw new Error('Missing env CONFIG_IN/CONFIG_OUT/TMP_DIR/INSTR_FILE');
          fs.mkdirSync(tmpDir, { recursive: true });
          const cfg = JSON.parse(fs.readFileSync(configIn, 'utf8'));
          cfg.featureFlags = { ...(cfg.featureFlags||{}), dataQa: 'active' };
          cfg.dataQA = { enabled: true, maxMissingBarsPerInstrument: 1, allowDuplicates: false, spikeZ: 5, repair: { forwardFillBars: 0, dropSpikes: true } };
          cfg.data = cfg.data || {};
          cfg.data.instrumentsFile = path.resolve(instrumentsFile);
          if (!cfg.data.ticks || typeof cfg.data.ticks !== 'object') throw new Error('config.data.ticks missing or not an object');
          const rewritten = {};
          // Choose one target symbol to introduce an interior gap exceeding K=1
          const tickSymbols = Object.keys(cfg.data.ticks || {});
          if (tickSymbols.length === 0) throw new Error('No tick symbols found in config.data.ticks');
          const targetSym = tickSymbols.find(s => /EURUSD/i.test(s)) || tickSymbols[0];
          for (const [sym, srcRaw] of Object.entries(cfg.data.ticks)) {
            if (typeof srcRaw !== 'string') continue;
            const src = path.resolve(srcRaw);
            if (!fs.existsSync(src)) throw new Error(`Tick file not found: ${src}`);
            const dst = path.join(tmpDir, `ticks_${sym}.csv`);
            const lines = fs.readFileSync(src, 'utf8').split(/\r?\n/);
            const header = lines[0] ?? '';
            const data = lines.slice(1).filter(l => l.trim().length > 0);
            let dataAfter = data;
            if (kMode === 'overflow' && sym === targetSym) {
              // Remove two interior rows to create a mid-sequence gap; avoid edges so union still spans these times
              if (data.length > 5) {
                const mid = Math.floor(data.length / 2);
                const removeIdx = new Set([mid - 1, mid]);
                console.log(`[overflow] Target ${sym}: removing interior indices ${mid-1} and ${mid} of ${data.length}`);
                dataAfter = data.filter((_, i) => !removeIdx.has(i));
              } else if (data.length > 3) {
                console.log(`[overflow] Target ${sym}: short series, removing indices 1 and 2 of ${data.length}`);
                const removeIdx = new Set([1, 2]);
                dataAfter = data.filter((_, i) => !removeIdx.has(i));
              } else if (data.length > 2) {
                console.log(`[overflow] Target ${sym}: very short series, removing index 1 of ${data.length}`);
                const removeIdx = new Set([1]);
                dataAfter = data.filter((_, i) => !removeIdx.has(i));
              } else {
                // Too short to meaningfully remove interior rows; leave as-is to avoid empty file
                console.warn(`[overflow] Target ${sym}: insufficient rows (${data.length}) to create interior gap; leaving data unchanged`);
              }
            } else if (kMode !== 'overflow') {
              // For completeness if ever reused: remove one from start in non-overflow (not used here)
              dataAfter = data.slice(1);
            }
            const out = [header, ...dataAfter].join('\n') + '\n';
            fs.writeFileSync(dst, out, 'utf8');
            const verify = fs.readFileSync(dst, 'utf8').trim().split(/\r?\n/);
            if (verify.length < 2) {
              console.error(`After K edit, ${dst} has no data rows (lines=${verify.length}).`);
              process.exit(1);
            }
            rewritten[sym] = path.resolve(dst);
          }
          cfg.data.ticks = rewritten;
          cfg.output = cfg.output || {};
          cfg.output.journalDir = path.resolve(path.join(tmpDir, 'journals'));
          cfg.name = cfg.name || 'backtest-m0';
          fs.writeFileSync(configOut, JSON.stringify(cfg, null, 2), 'utf8');
          fs.appendFileSync(process.env.GITHUB_ENV, `\nCONFIG_OUT=${configOut}\n`);
          fs.appendFileSync(process.env.GITHUB_ENV, `TMP_DIR=${tmpDir}\n`);
          NODE
          # Debug: list temp dir and first 3 lines of each rewritten CSV
          echo "Temp dir listing:" && ls -la "$TMP_DIR"
          for f in "$TMP_DIR"/ticks_*.csv; do echo "== $f =="; head -n 3 "$f"; done
          # Run sim with prepared config and capture stdout
          OUT=$(mktemp)
          set +e
          dotnet exec "$sim" --config "$CONFIG_OUT" --run-id K1-FAIL --verbosity quiet | tee "$OUT"
          SIM_CODE=${PIPESTATUS[0]}
          set -e
          if [ $SIM_CODE -ne 0 ]; then
            echo "Simulator exited with $SIM_CODE" >&2
            echo '--- simulator stdout (K1-FAIL) ---'
            cat "$OUT"
            exit $SIM_CODE
          fi
          J_EVENTS=$(grep -m1 '^JOURNAL_DIR_EVENTS=' "$OUT" | cut -d= -f2 || true)
          J_TRADES=$(grep -m1 '^JOURNAL_DIR_TRADES=' "$OUT" | cut -d= -f2 || true)
          if [ -z "$J_EVENTS" ]; then
            echo "Missing JOURNAL_DIR_EVENTS entry for K1-FAIL" >&2
            echo '--- simulator stdout (K1-FAIL) ---'
            cat "$OUT"
            exit 2
          fi
          cp "$OUT" artifacts/dataqa/fail/sim.log
          printf 'mode=fail\nrun_dir=%s\nevents=%s\ntrades=%s\n' "$(dirname "$J_EVENTS")" "$J_EVENTS" "$J_TRADES" >> artifacts/env/env.sanity
          cp "$J_EVENTS" artifacts/dataqa/fail/events.csv
          if [ -n "$J_TRADES" ] && [ -f "$J_TRADES" ]; then
            cp "$J_TRADES" artifacts/dataqa/fail/trades.csv
          fi
          # Assert summary passed=false (pure Node)
          EVENTS="$J_EVENTS" node - <<'JS'
          const fs=require("fs");
          const line=(fs.readFileSync(process.env.EVENTS,"utf8").split(/\r?\n/).find(l=>l.includes(",DATA_QA_SUMMARY_V1,"))||"");
          if(!line){ console.error("DATA_QA_SUMMARY_V1 not found"); process.exit(1); }
          const QUOTE=String.fromCharCode(34);
          function splitCsv(s){ const out=[]; let buf="", q=false; for(let i=0;i<s.length;i++){ const c=s[i]; if(q){ if(c===QUOTE){ if(i+1<s.length && s[i+1]===QUOTE){ buf+=QUOTE; i++; } else { q=false; } } else { buf+=c; } } else { if(c===','){ out.push(buf); buf=""; } else if(c===QUOTE){ q=true; } else { buf+=c; } } } out.push(buf); return out; }
          const cols=splitCsv(line);
          const payload=cols[3];
          try {
            const j=JSON.parse(payload);
            if(j.passed!==false){ console.error("Expected passed=false in DATA_QA_SUMMARY_V1", j); process.exit(1); }
          } catch(e) {
            console.error("Failed to parse payload as JSON:", payload); process.exit(1);
          }
          JS
          # Strict verify
          # If Data QA aborted, trades.csv may not exist. In that case, verify events only.
          tools_out=0
          if [ -n "$J_TRADES" ] && [ -f "$J_TRADES" ]; then
            dotnet exec "$tools" verify strict --events "$J_EVENTS" --trades "$J_TRADES" --schema 1.3.0 --json || tools_out=$?
          else
            echo "trades.csv missing (expected after QA abort); verifying events only"
            dotnet exec "$tools" verify strict --events "$J_EVENTS" --schema 1.3.0 --json || tools_out=$?
          fi
          if [ "$tools_out" -ne 0 ] && [ "$tools_out" -ne 2 ]; then exit 1; fi

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dataqa-tolerance-${{ matrix.os }}
          path: artifacts/**
